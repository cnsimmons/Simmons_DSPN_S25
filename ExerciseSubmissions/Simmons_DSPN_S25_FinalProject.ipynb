{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrated Networks: Analyzing Dorsal and Ventral Visual Pathway Connectivity in Object Recognition\n",
    "\n",
    "## Background\n",
    "\n",
    "The human brain processes visual information through two primary pathways: the ventral \"what\" pathway, critical for recognizing and identifying objects, and the dorsal \"where/how\" pathway, traditionally associated with spatial processing and action guidance. For decades, these pathways were viewed as distinct processing streams with separate functions. However, recent research suggests they may be more integrated than previously thought.\n",
    "\n",
    "This integration matters beyond neuroscience because it provides insight into how our brain combines object recognition with action planning - a critical function for technologies like computer vision, robotics, and treatments for visual processing disorders. By analyzing functional connectivity patterns between key regions in these pathways (lateral occipital complex in the ventral stream and posterior intraparietal sulcus in the dorsal stream), this study aims to determine whether these pathways function as separate networks or as an integrated system during object recognition.\n",
    "\n",
    "## Variables\n",
    "\n",
    "- **Participants**: 18 right-handed individuals (10 female, mean age = 17.56 years) with normal or corrected-to-normal vision\n",
    "- **Data Type**: Functional Magnetic Resonance Imaging (fMRI) data\n",
    "- **Regions of Interest (ROIs)**: \n",
    "  - **Lateral Occipital Complex (LO)**: Categorical, ventral \"what\" pathway region\n",
    "  - **Posterior Intraparietal Sulcus (pIPS)**: Categorical, dorsal \"where/how\" pathway region\n",
    "- **Connectivity Measures**: \n",
    "  - **Dice Coefficient**: Continuous (0-1), measuring spatial overlap between connectivity networks\n",
    "  - **Connectivity Fingerprints**: Continuous values, correlation coefficients between ROI time series and atlas regions\n",
    "  - **Vector Correlations**: Continuous (-1 to 1), correlation between connectivity profiles\n",
    "- **Data Acquisition Parameters**:\n",
    "  - 69 slices\n",
    "  - TR = 2000ms\n",
    "  - TE = 30ms\n",
    "  - Flip angle = 79°\n",
    "  - Voxel size = 2×2×2mm³\n",
    "  - FOV = 212mm\n",
    "  - Multi-band acceleration factor = 3\n",
    "\n",
    "## Hypotheses\n",
    "\n",
    "We hypothesize that dorsal (pIPS) and ventral (LO) visual pathways function as a single integrated network rather than two separate networks during object recognition. We can express this through the following specific hypotheses and models:\n",
    "\n",
    "1. **Spatial Overlap Hypothesis**: Within-subject spatial overlap between dorsal and ventral networks will be significantly higher than between-subject overlap for either network.\n",
    "   - Model: $\\text{Dice}_{\\text{within-subject}} > \\text{Dice}_{\\text{between-subject}}$\n",
    "   - Statistical test: Repeated measures ANOVA comparing Dice coefficients\n",
    "\n",
    "2. **Connectivity Profile Hypothesis**: Despite functioning as an integrated network, we predict distinct connectivity profiles for each pathway, with the dorsal pathway showing more extensive connectivity.\n",
    "   - Model: $\\text{Number of connections}_{\\text{pIPS}} \\neq \\text{Number of connections}_{\\text{LO}}$\n",
    "   - Statistical test: Chi-square test for connection count asymmetry\n",
    "\n",
    "3. **Connection Strength Hypothesis**: While connection count may differ, we predict similar connection strengths for both pathways.\n",
    "   - Model: $\\text{Mean strength}_{\\text{pIPS}} \\approx \\text{Mean strength}_{\\text{LO}}$\n",
    "   - Statistical test: Mann-Whitney U test comparing connection strength distributions\n",
    "\n",
    "## Data Organization\n",
    "\n",
    "The data for this analysis was organized in the following structure:\n",
    "\n",
    "- **Raw Data**: fMRI time series for 18 participants performing an object processing task\n",
    "- **Processed Data**: \n",
    "  - ROI-based functional connectivity maps in MNI space\n",
    "  - Left and right hemisphere data for each ROI (pIPS and LO)\n",
    "  - Atlas-based connectivity measures using a merged Schaefer-Wang atlas\n",
    "  \n",
    "**Final Data Table Structure:**\n",
    "\n",
    "| Subject ID | ROI | Hemisphere | Region | Connectivity Value |\n",
    "|------------|-----|------------|--------|-------------------|\n",
    "| sub-001    | pIPS | Left      | Region1| 0.42              |\n",
    "| sub-001    | pIPS | Right     | Region1| 0.39              |\n",
    "| sub-001    | LO   | Left      | Region1| 0.36              |\n",
    "| ...        | ...  | ...       | ...    | ...               |\n",
    "\n",
    "This organization allows for:\n",
    "1. Within-subject comparisons between ROIs (pIPS vs. LO)\n",
    "2. Between-subject comparisons within ROIs\n",
    "3. Hemisphere-specific and bilateral analyses\n",
    "4. Region-by-region connectivity fingerprint analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn import image, datasets, plotting\n",
    "from nilearn.maskers import NiftiLabelsMasker\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Define base paths\n",
    "base_dir = '/user_data/csimmon2/git_repos/ptoc'\n",
    "roi_dir = f'{base_dir}/roiParcels'\n",
    "results_dir = f'{base_dir}/results'\n",
    "group_out_dir = f'{results_dir}/group_averages'\n",
    "study_dir = \"/lab_data/behrmannlab/vlad/ptoc\"\n",
    "sub_info_path = '/user_data/csimmon2/git_repos/ptoc/sub_info.csv'\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "os.makedirs(group_out_dir, exist_ok=True)\n",
    "\n",
    "# Define analysis-specific directories\n",
    "connectivity_dir = f'{results_dir}/connectivity_comparison'\n",
    "dice_output_dir = f'{results_dir}/dice_comparison'\n",
    "asymmetry_output_dir = f'{results_dir}/connectivity_asymmetry'\n",
    "\n",
    "# Create analysis-specific directories\n",
    "os.makedirs(connectivity_dir, exist_ok=True)\n",
    "os.makedirs(dice_output_dir, exist_ok=True)\n",
    "os.makedirs(asymmetry_output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Our analysis employed three primary methods to investigate the network organization of object recognition in the human brain:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Dice Coefficient Analysis\n",
    "\n",
    "We used Dice coefficients to quantify the spatial overlap between connectivity networks, allowing us to directly test our spatial overlap hypothesis. The Dice coefficient is calculated as:\n",
    "\n",
    "$$\\text{Dice} = \\frac{2|X \\cap Y|}{|X| + |Y|}$$\n",
    "\n",
    "where $X$ and $Y$ represent the binarized connectivity maps being compared.\n",
    "\n",
    "This analysis revealed remarkably high within-subject overlap between dorsal (pIPS) and ventral (LO) networks (mean Dice coefficient = 0.91), compared to between-subject overlap for either dorsal pIPS (0.83) or ventral LO (0.79) regions. \n",
    "\n",
    "Statistical analysis using repeated measures ANOVA confirmed significant differences among the three comparison types (F = 33.54, p < 0.001), with post-hoc tests confirming that within-subject overlap between pIPS and LO is significantly higher than between-subject overlap for either pIPS or LO (all p-values < 0.01)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dice Coefficient Analysis Script\n",
    "def dice_coefficient(img1_data, img2_data):\n",
    "    \"\"\"\n",
    "    Calculate Dice coefficient between two binary arrays.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    img1_data : numpy.ndarray\n",
    "        First binary image data\n",
    "    img2_data : numpy.ndarray\n",
    "        Second binary image data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Dice coefficient value between 0 (no overlap) and 1 (perfect overlap)\n",
    "    \"\"\"\n",
    "    # Ensure binary\n",
    "    img1_bin = (img1_data > 0).astype(int)\n",
    "    img2_bin = (img2_data > 0).astype(int)\n",
    "    \n",
    "    # Calculate intersection\n",
    "    intersection = np.sum(img1_bin * img2_bin)\n",
    "    \n",
    "    # Calculate Dice using formula: 2*intersection/(sum of elements)\n",
    "    dice = 2.0 * intersection / (np.sum(img1_bin) + np.sum(img2_bin))\n",
    "    \n",
    "    return dice\n",
    "\n",
    "def analyze_data_type(analysis_type):\n",
    "    \"\"\"\n",
    "    Analyze dice coefficients for a specific data type (fc or ppi)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    analysis_type : str\n",
    "        Type of analysis to perform ('fc' for functional connectivity)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing results of the analysis\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ANALYZING {analysis_type.upper()} MAPS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Load subject info\n",
    "    sub_info = pd.read_csv(sub_info_path)\n",
    "    subjects = sub_info[sub_info['group'] == 'control']['sub'].tolist()\n",
    "    \n",
    "    # Exclude sub-084 as specified\n",
    "    if 'sub-084' in subjects:\n",
    "        subjects.remove('sub-084')\n",
    "        print(\"Excluded sub-084 from analysis\")\n",
    "    \n",
    "    print(f\"Found {len(subjects)} control subjects\")\n",
    "    \n",
    "    # Define ROIs and hemispheres\n",
    "    rois = ['pIPS', 'LO']\n",
    "    hemispheres = ['left', 'right']\n",
    "    \n",
    "    # Load and preprocess subject data\n",
    "    subject_data = {}\n",
    "    valid_subjects = []\n",
    "    \n",
    "    for sub in subjects:\n",
    "        has_all_data = True\n",
    "        subject_data[sub] = {}\n",
    "        \n",
    "        for roi in rois:\n",
    "            # Initialize lists to store hemisphere data\n",
    "            roi_data_arrays = []\n",
    "            \n",
    "            for hemi in hemispheres:\n",
    "                # Both FC and PPI files are in the same directory but with different analysis_type in the filename\n",
    "                roi_file = f'{study_dir}/{sub}/ses-01/derivatives/fc_mni/{sub}_{roi}_{hemi}_loc_{analysis_type}_mni.nii.gz'\n",
    "                \n",
    "                if os.path.exists(roi_file):\n",
    "                    try:\n",
    "                        img = nib.load(roi_file)\n",
    "                        roi_data_arrays.append(img.get_fdata())\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading {roi_file}: {e}\")\n",
    "                        has_all_data = False\n",
    "                else:\n",
    "                    print(f\"File not found: {roi_file}\")\n",
    "                    has_all_data = False\n",
    "            \n",
    "            # Average the hemispheres if we have data for both\n",
    "            if len(roi_data_arrays) == 2:\n",
    "                # Create averaged map\n",
    "                avg_data = (roi_data_arrays[0] + roi_data_arrays[1]) / 2\n",
    "                subject_data[sub][roi] = avg_data\n",
    "            else:\n",
    "                has_all_data = False\n",
    "        \n",
    "        if has_all_data:\n",
    "            valid_subjects.append(sub)\n",
    "    \n",
    "    print(f\"Found {len(valid_subjects)} subjects with complete {analysis_type} data\")\n",
    "    \n",
    "    if len(valid_subjects) == 0:\n",
    "        print(f\"No valid subjects found for {analysis_type} analysis. Skipping.\")\n",
    "        return None\n",
    "    \n",
    "    # 1. Within-subject, between-ROI analysis (pIPS vs LO)\n",
    "    print(f\"\\nCalculating within-subject, between-ROI dice coefficients for {analysis_type}...\")\n",
    "    between_roi_results = []\n",
    "    \n",
    "    for sub in valid_subjects:\n",
    "        try:\n",
    "            # Calculate Dice coefficient\n",
    "            dice_score = dice_coefficient(subject_data[sub]['pIPS'], subject_data[sub]['LO'])\n",
    "            \n",
    "            between_roi_results.append({\n",
    "                'Subject': sub,\n",
    "                'Analysis': analysis_type,\n",
    "                'Dice': dice_score\n",
    "            })\n",
    "            \n",
    "            print(f\"Subject {sub}, {analysis_type}, pIPS vs LO: {dice_score:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing subject {sub}: {e}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    between_roi_df = pd.DataFrame(between_roi_results)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    mean = between_roi_df['Dice'].mean()\n",
    "    std = between_roi_df['Dice'].std()\n",
    "    # 95% confidence interval\n",
    "    ci = stats.t.interval(0.95, len(between_roi_df)-1, loc=mean, scale=std/np.sqrt(len(between_roi_df)))\n",
    "    \n",
    "    print(f\"\\n{analysis_type.upper()}: pIPS vs LO within-subject similarity:\")\n",
    "    print(f\"Mean Dice: {mean:.4f} ± {std:.4f}\")\n",
    "    print(f\"95% CI: [{ci[0]:.4f}, {ci[1]:.4f}]\")\n",
    "    print(f\"N = {len(between_roi_df)} subjects\")\n",
    "    \n",
    "    # 2. Between-subject, within-ROI analysis\n",
    "    print(f\"\\nCalculating between-subject, within-ROI dice coefficients for {analysis_type}...\")\n",
    "    within_roi_results = []\n",
    "    \n",
    "    for roi in rois:\n",
    "        # Compare each subject to all others\n",
    "        for i, sub1 in enumerate(valid_subjects):\n",
    "            for j, sub2 in enumerate(valid_subjects):\n",
    "                if i < j:  # Only compare each pair once\n",
    "                    try:\n",
    "                        # Calculate Dice coefficient\n",
    "                        dice_score = dice_coefficient(subject_data[sub1][roi], subject_data[sub2][roi])\n",
    "                        \n",
    "                        within_roi_results.append({\n",
    "                            'ROI': roi,\n",
    "                            'Analysis': analysis_type,\n",
    "                            'Subject1': sub1,\n",
    "                            'Subject2': sub2,\n",
    "                            'Dice': dice_score\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error comparing {sub1} and {sub2} for {roi}: {e}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    within_roi_df = pd.DataFrame(within_roi_results)\n",
    "    \n",
    "    # Calculate statistics for each ROI\n",
    "    within_roi_stats = []\n",
    "    \n",
    "    for roi in rois:\n",
    "        roi_data = within_roi_df[within_roi_df['ROI'] == roi]\n",
    "        \n",
    "        if len(roi_data) > 0:\n",
    "            mean = roi_data['Dice'].mean()\n",
    "            std = roi_data['Dice'].std()\n",
    "            # 95% confidence interval\n",
    "            ci = stats.t.interval(0.95, len(roi_data)-1, loc=mean, scale=std/np.sqrt(len(roi_data)))\n",
    "            \n",
    "            within_roi_stats.append({\n",
    "                'ROI': roi,\n",
    "                'Analysis': analysis_type,\n",
    "                'Mean Dice': mean,\n",
    "                'Std Dev': std,\n",
    "                '95% CI Lower': ci[0],\n",
    "                '95% CI Upper': ci[1],\n",
    "                'N': len(roi_data)\n",
    "            })\n",
    "            \n",
    "            print(f\"\\n{analysis_type.upper()}: {roi} between-subject similarity:\")\n",
    "            print(f\"Mean Dice: {mean:.4f} ± {std:.4f}\")\n",
    "            print(f\"95% CI: [{ci[0]:.4f}, {ci[1]:.4f}]\")\n",
    "            print(f\"Based on {len(roi_data)} comparisons among {len(valid_subjects)} subjects\")\n",
    "    \n",
    "    # Save results to CSV files\n",
    "    between_roi_df.to_csv(f'{output_dir}/{analysis_type}_between_roi_dice_by_subject.csv', index=False)\n",
    "    within_roi_df.to_csv(f'{output_dir}/{analysis_type}_within_roi_dice_by_pair.csv', index=False)\n",
    "    \n",
    "    within_roi_stats_df = pd.DataFrame(within_roi_stats)\n",
    "    within_roi_stats_df.to_csv(f'{output_dir}/{analysis_type}_within_roi_stats.csv', index=False)\n",
    "    \n",
    "    # Prepare summary data\n",
    "    between_roi_stats = {\n",
    "        'ROI': 'pIPS vs LO',\n",
    "        'Analysis': analysis_type,\n",
    "        'Mean Dice': between_roi_df['Dice'].mean(),\n",
    "        'Std Dev': between_roi_df['Dice'].std(),\n",
    "        '95% CI Lower': ci[0],\n",
    "        '95% CI Upper': ci[1],\n",
    "        'N': len(between_roi_df)\n",
    "    }\n",
    "    \n",
    "    # Combine all stats into one DataFrame\n",
    "    all_stats = pd.DataFrame([between_roi_stats] + within_roi_stats)\n",
    "    \n",
    "    return {\n",
    "        'between_roi': between_roi_df,\n",
    "        'within_roi': within_roi_df,\n",
    "        'stats': all_stats\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Analyze FC data and calculate summary statistics\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    all_stats = []\n",
    "    \n",
    "    # Analyze FC data\n",
    "    print(\"\\nAnalyzing FC data...\")\n",
    "    fc_results = analyze_data_type('fc')\n",
    "    if fc_results:\n",
    "        results['fc'] = fc_results\n",
    "        all_stats.append(fc_results['stats'])\n",
    "    \n",
    "    # Combine stats from the analysis\n",
    "    if all_stats:\n",
    "        combined_stats = pd.concat(all_stats)\n",
    "        combined_stats.to_csv(f'{output_dir}/combined_dice_stats.csv', index=False)\n",
    "        \n",
    "        # Print combined summary\n",
    "        print(\"\\n\\nCOMBINED SUMMARY OF DICE COEFFICIENT ANALYSES\")\n",
    "        print(\"=\" * 80)\n",
    "        print(combined_stats.to_string(index=False))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the Dice coefficient analysis\n",
    "results = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare within-ROI vs between-ROI dice coefficients using ANOVA\n",
    "def compare_within_vs_between_dice_anova_averaged(analysis_type='fc'):\n",
    "    \"\"\"\n",
    "    Compare within-ROI vs between-ROI dice coefficients using ANOVA with subject-level averages\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    analysis_type : str\n",
    "        Type of analysis to perform ('fc' for functional connectivity)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing results of the ANOVA analysis\n",
    "    \"\"\"\n",
    "    # Define study directories\n",
    "    study_dir = \"/lab_data/behrmannlab/vlad/ptoc\"\n",
    "    results_dir = '/user_data/csimmon2/git_repos/ptoc/results'\n",
    "    sub_info_path = '/user_data/csimmon2/git_repos/ptoc/sub_info.csv'\n",
    "    output_dir = f'{results_dir}/dice_comparison'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"Comparing within-ROI vs between-ROI dice coefficients using ANOVA with subject averages for {analysis_type.upper()}...\")\n",
    "    \n",
    "    # Load subject info\n",
    "    sub_info = pd.read_csv(sub_info_path)\n",
    "    subjects = sub_info[sub_info['group'] == 'control']['sub'].tolist()\n",
    "    \n",
    "    # Exclude sub-084 as specified\n",
    "    if 'sub-084' in subjects:\n",
    "        subjects.remove('sub-084')\n",
    "        print(\"Excluded sub-084 from analysis\")\n",
    "    \n",
    "    print(f\"Found {len(subjects)} control subjects\")\n",
    "    \n",
    "    # Define ROIs and hemispheres\n",
    "    rois = ['pIPS', 'LO']\n",
    "    hemispheres = ['left', 'right']\n",
    "    \n",
    "    # Load and preprocess subject data\n",
    "    subject_data = {}\n",
    "    valid_subjects = []\n",
    "    \n",
    "    for sub in subjects:\n",
    "        has_all_data = True\n",
    "        subject_data[sub] = {}\n",
    "        \n",
    "        for roi in rois:\n",
    "            # Initialize lists to store hemisphere data\n",
    "            roi_data_arrays = []\n",
    "            \n",
    "            for hemi in hemispheres:\n",
    "                roi_file = f'{study_dir}/{sub}/ses-01/derivatives/fc_mni/{sub}_{roi}_{hemi}_loc_{analysis_type}_mni.nii.gz'\n",
    "                \n",
    "                if os.path.exists(roi_file):\n",
    "                    try:\n",
    "                        img = nib.load(roi_file)\n",
    "                        roi_data_arrays.append(img.get_fdata())\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading {roi_file}: {e}\")\n",
    "                        has_all_data = False\n",
    "                else:\n",
    "                    print(f\"File not found: {roi_file}\")\n",
    "                    has_all_data = False\n",
    "            \n",
    "            # Average the hemispheres if we have data for both\n",
    "            if len(roi_data_arrays) == 2:\n",
    "                # Create averaged map\n",
    "                avg_data = (roi_data_arrays[0] + roi_data_arrays[1]) / 2\n",
    "                subject_data[sub][roi] = (avg_data > 0).astype(int)  # Binarize\n",
    "            else:\n",
    "                has_all_data = False\n",
    "        \n",
    "        if has_all_data:\n",
    "            valid_subjects.append(sub)\n",
    "    \n",
    "    print(f\"Found {len(valid_subjects)} subjects with complete {analysis_type} data\")\n",
    "    \n",
    "    if len(valid_subjects) == 0:\n",
    "        print(f\"No valid subjects found for {analysis_type} analysis. Skipping.\")\n",
    "        return None\n",
    "    \n",
    "    # 1. Within-subject, between-ROI analysis (pIPS vs LO)\n",
    "    print(f\"\\nCalculating within-subject, between-ROI dice coefficients for {analysis_type}...\")\n",
    "    between_roi_results = []\n",
    "    \n",
    "    for sub in valid_subjects:\n",
    "        try:\n",
    "            # Get binary maps\n",
    "            pips_bin = subject_data[sub]['pIPS']\n",
    "            lo_bin = subject_data[sub]['LO']\n",
    "            \n",
    "            # Calculate intersection\n",
    "            intersection = np.sum(pips_bin * lo_bin)\n",
    "            \n",
    "            # Calculate Dice\n",
    "            dice_score = 2.0 * intersection / (np.sum(pips_bin) + np.sum(lo_bin))\n",
    "            \n",
    "            between_roi_results.append({\n",
    "                'Subject': sub,\n",
    "                'Dice': dice_score,\n",
    "                'Comparison': 'Within-subject_between-ROI'\n",
    "            })\n",
    "            \n",
    "            print(f\"Subject {sub}, pIPS vs LO: {dice_score:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing subject {sub}: {e}\")\n",
    "    \n",
    "    # 2. Calculate all pairwise between-subject dice coefficients\n",
    "    print(f\"\\nCalculating pairwise between-subject, within-ROI dice coefficients for {analysis_type}...\")\n",
    "    pairwise_results = {}\n",
    "    \n",
    "    for roi in rois:\n",
    "        pairwise_results[roi] = {}\n",
    "        # Initialize the nested dictionary structure\n",
    "        for sub in valid_subjects:\n",
    "            pairwise_results[roi][sub] = {}\n",
    "        \n",
    "        # Compare each subject to all others\n",
    "        for i, sub1 in enumerate(valid_subjects):\n",
    "            for j, sub2 in enumerate(valid_subjects):\n",
    "                if i < j:  # Only compare each pair once\n",
    "                    try:\n",
    "                        # Get binary maps\n",
    "                        map1 = subject_data[sub1][roi]\n",
    "                        map2 = subject_data[sub2][roi]\n",
    "                        \n",
    "                        # Calculate intersection\n",
    "                        intersection = np.sum(map1 * map2)\n",
    "                        \n",
    "                        # Calculate Dice\n",
    "                        dice_score = 2.0 * intersection / (np.sum(map1) + np.sum(map2))\n",
    "                        \n",
    "                        # Store the result for both subjects\n",
    "                        pairwise_results[roi][sub1][sub2] = dice_score\n",
    "                        pairwise_results[roi][sub2][sub1] = dice_score\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error comparing {sub1} and {sub2} for {roi}: {e}\")\n",
    "    \n",
    "    # 3. Average the between-subject dice coefficients for each subject\n",
    "    print(f\"\\nAveraging between-subject dice coefficients for each subject for {analysis_type}...\")\n",
    "    average_results = []\n",
    "    \n",
    "    for roi in rois:\n",
    "        for sub in pairwise_results[roi]:\n",
    "            # Calculate the average dice with all other subjects\n",
    "            if len(pairwise_results[roi][sub]) > 0:  # Ensure there are values to average\n",
    "                avg_dice = np.mean(list(pairwise_results[roi][sub].values()))\n",
    "                \n",
    "                average_results.append({\n",
    "                    'Subject': sub,\n",
    "                    'ROI': roi,\n",
    "                    'Dice': avg_dice,\n",
    "                    'Comparison': f'Between-subject_within-{roi}'\n",
    "                })\n",
    "                print(f\"Subject {sub}, average {roi} similarity with other subjects: {avg_dice:.4f}\")\n",
    "    \n",
    "    # Combine results for ANOVA\n",
    "    all_results = pd.DataFrame(between_roi_results)\n",
    "    average_df = pd.DataFrame(average_results)\n",
    "    \n",
    "    # Create separate DataFrames for pIPS and LO\n",
    "    pips_df = average_df[average_df['ROI'] == 'pIPS'].copy()\n",
    "    lo_df = average_df[average_df['ROI'] == 'LO'].copy()\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    print(f\"\\nSummary Statistics for {analysis_type.upper()}:\")\n",
    "    \n",
    "    groups = {\n",
    "        'Within-subject, between-ROI': all_results['Dice'].values,\n",
    "        'Between-subject, within-pIPS': pips_df['Dice'].values,\n",
    "        'Between-subject, within-LO': lo_df['Dice'].values\n",
    "    }\n",
    "    \n",
    "    for group_name, values in groups.items():\n",
    "        if len(values) > 0:\n",
    "            mean = np.mean(values)\n",
    "            std = np.std(values)\n",
    "            ci = stats.t.interval(0.95, len(values)-1, loc=mean, scale=std/np.sqrt(len(values)))\n",
    "            print(f\"{group_name}:\")\n",
    "            print(f\"  N = {len(values)}\")\n",
    "            print(f\"  Mean = {mean:.4f} ± {std:.4f}\")\n",
    "            print(f\"  95% CI = [{ci[0]:.4f}, {ci[1]:.4f}]\")\n",
    "    \n",
    "    # Save summary statistics to CSV\n",
    "    summary_stats = []\n",
    "    for group_name, values in groups.items():\n",
    "        if len(values) > 0:\n",
    "            mean = np.mean(values)\n",
    "            std = np.std(values)\n",
    "            ci = stats.t.interval(0.95, len(values)-1, loc=mean, scale=std/np.sqrt(len(values)))\n",
    "            summary_stats.append({\n",
    "                'Analysis': analysis_type,\n",
    "                'Group': group_name,\n",
    "                'N': len(values),\n",
    "                'Mean': mean,\n",
    "                'Std': std,\n",
    "                'CI_Lower': ci[0],\n",
    "                'CI_Upper': ci[1]\n",
    "            })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_stats)\n",
    "    summary_df.to_csv(f'{output_dir}/{analysis_type}_anova_summary_stats.csv', index=False)\n",
    "    \n",
    "    # Prepare data for ANOVA - now all groups should have the same sample size based on valid subjects\n",
    "    anova_data = pd.DataFrame({\n",
    "        'Dice': np.concatenate([\n",
    "            all_results['Dice'].values,\n",
    "            pips_df['Dice'].values,\n",
    "            lo_df['Dice'].values\n",
    "        ]),\n",
    "        'Group': np.concatenate([\n",
    "            ['Within-subject_between-ROI'] * len(all_results),\n",
    "            ['Between-subject_within-pIPS'] * len(pips_df),\n",
    "            ['Between-subject_within-LO'] * len(lo_df)\n",
    "        ])\n",
    "    })\n",
    "    \n",
    "    # Run One-way ANOVA\n",
    "    print(f\"\\nPerforming One-way ANOVA with subject averages for {analysis_type}...\")\n",
    "    import statsmodels.api as sm\n",
    "    from statsmodels.formula.api import ols\n",
    "    \n",
    "    model = ols('Dice ~ C(Group)', data=anova_data).fit()\n",
    "    anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "    \n",
    "    print(\"\\nANOVA Results:\")\n",
    "    print(anova_table)\n",
    "    \n",
    "    # Save ANOVA results to CSV\n",
    "    anova_df = pd.DataFrame(anova_table)\n",
    "    anova_df.to_csv(f'{output_dir}/{analysis_type}_anova_results.csv')\n",
    "    \n",
    "    # Calculate pairwise t-tests and apply Holm-Bonferroni correction\n",
    "    unique_groups = anova_data['Group'].unique()\n",
    "    p_values = []\n",
    "    pairs = []\n",
    "\n",
    "    for i, group1 in enumerate(unique_groups):\n",
    "        for j, group2 in enumerate(unique_groups):\n",
    "            if i < j:  # Compare each pair only once\n",
    "                data1 = anova_data[anova_data['Group'] == group1]['Dice']\n",
    "                data2 = anova_data[anova_data['Group'] == group2]['Dice']\n",
    "                _, p_val = stats.ttest_ind(data1, data2, equal_var=False)\n",
    "                p_values.append(p_val)\n",
    "                pairs.append((group1, group2))\n",
    "\n",
    "    # Apply Holm-Bonferroni correction\n",
    "    from statsmodels.stats.multitest import multipletests\n",
    "    reject, p_corrected, _, _ = multipletests(p_values, alpha=0.05, method='holm')\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\nHolm-Bonferroni Results:\")\n",
    "    for i, ((group1, group2), p_corr, rej) in enumerate(zip(pairs, p_corrected, reject)):\n",
    "        print(f\"{group1} vs {group2}: p-corrected = {p_corr:.4f}, significant: {rej}\")\n",
    "\n",
    "    # Save results to a text file\n",
    "    with open(f'{output_dir}/{analysis_type}_holm_bonferroni_results.txt', 'w') as f:\n",
    "        f.write(\"Holm-Bonferroni corrected pairwise comparisons:\\n\\n\")\n",
    "        for i, ((group1, group2), p_corr, rej) in enumerate(zip(pairs, p_corrected, reject)):\n",
    "            f.write(f\"{group1} vs {group2}: p-corrected = {p_corr:.4f}, significant: {rej}\\n\")\n",
    "    \n",
    "    # Print interpretation\n",
    "    print(f\"\\nINTERPRETATION for {analysis_type.upper()}:\")\n",
    "    \n",
    "    # Get F-value and p-value from ANOVA table\n",
    "    f_value = anova_table.loc['C(Group)', 'F']\n",
    "    p_value = anova_table.loc['C(Group)', 'PR(>F)']\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(f\"The ANOVA shows significant differences in Dice coefficients among the three comparison types (F={f_value:.4f}, p={p_value:.4f}).\")\n",
    "        print(\"The Holm Bonferroni post-hoc test shows which specific groups differ from each other.\")\n",
    "        print(\"\\nBased on the means:\")\n",
    "        \n",
    "        means = {\n",
    "            'Within-subject_between-ROI': np.mean(groups['Within-subject, between-ROI']),\n",
    "            'Between-subject_within-pIPS': np.mean(groups['Between-subject, within-pIPS']),\n",
    "            'Between-subject_within-LO': np.mean(groups['Between-subject, within-LO'])\n",
    "        }\n",
    "        \n",
    "        ordered_groups = sorted(means.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        for i, (group, mean) in enumerate(ordered_groups):\n",
    "            print(f\"{i+1}. {group}: {mean:.4f}\")\n",
    "        \n",
    "        if means['Within-subject_between-ROI'] > means['Between-subject_within-pIPS'] and means['Within-subject_between-ROI'] > means['Between-subject_within-LO']:\n",
    "            print(\"\\nThis suggests that different ROIs within the same subject share more similar connectivity patterns\")\n",
    "            print(\"than the same ROI across different subjects. This pattern holds for both pIPS and LO.\")\n",
    "        else:\n",
    "            print(\"\\nThe pattern suggests a more complex relationship. Please refer to the Holm-Bon results for specific group differences.\")\n",
    "    else:\n",
    "        print(f\"The ANOVA shows no significant differences in Dice coefficients among the three comparison types (F={f_value:.4f}, p={p_value:.4f}).\")\n",
    "        print(\"This suggests that the level of similarity does not significantly differ between:\")\n",
    "        print(\"- different ROIs within the same subject\")\n",
    "        print(\"- the same ROI across different subjects\")\n",
    "    \n",
    "    return {\n",
    "        'anova_table': anova_table,\n",
    "        'holm_result': {\n",
    "            'pairs': pairs,\n",
    "            'p_corrected': p_corrected,\n",
    "            'reject': reject\n",
    "        },\n",
    "        'group_means': {\n",
    "            'Within-subject_between-ROI': np.mean(groups['Within-subject, between-ROI']),\n",
    "            'Between-subject_within-pIPS': np.mean(groups['Between-subject, within-pIPS']),\n",
    "            'Between-subject_within-LO': np.mean(groups['Between-subject, within-LO'])\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Run ANOVA comparison for FC data\n",
    "fc_results = compare_within_vs_between_dice_anova_averaged('fc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Merged Atlas\n",
    "base_dir = '/user_data/csimmon2/git_repos/ptoc'\n",
    "roi_dir = f'{base_dir}/roiParcels'\n",
    "results_dir = f'{base_dir}/results'\n",
    "group_out_dir = f'{results_dir}/group_averages'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "os.makedirs(connectivity_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def create_merged_atlas():\n",
    "    \"\"\"\n",
    "    Create a merged atlas where Wang ROIs replace overlapping regions in Schaefer atlas\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing merged atlas information\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Step 1: Creating Merged Atlas ===\")\n",
    "    \n",
    "    # Load Wang ROIs - pIPS and LO\n",
    "    roi_files = {\n",
    "        'pIPS': f'{roi_dir}/pIPS.nii.gz',\n",
    "        'LO': f'{roi_dir}/LO.nii.gz'\n",
    "    }\n",
    "    \n",
    "    rois = {}\n",
    "    for roi_name, roi_path in roi_files.items():\n",
    "        if os.path.exists(roi_path):\n",
    "            rois[roi_name] = nib.load(roi_path)\n",
    "            print(f\"Loaded {roi_name} ROI\")\n",
    "        else:\n",
    "            print(f\"Error: ROI file {roi_path} not found!\")\n",
    "            return None\n",
    "    \n",
    "    # Load Schaefer atlas\n",
    "    atlas = datasets.fetch_atlas_schaefer_2018(n_rois=200, yeo_networks=7, resolution_mm=2)\n",
    "    atlas_img = nib.load(atlas.maps)\n",
    "    atlas_labels = atlas.labels\n",
    "    print(f\"Loaded Schaefer atlas with {len(atlas_labels)} parcels\")\n",
    "    \n",
    "    # Get atlas data\n",
    "    atlas_data = atlas_img.get_fdata()\n",
    "    modified_atlas_data = atlas_data.copy()\n",
    "    \n",
    "    # Create a dictionary to store new labels\n",
    "    new_labels = list(atlas_labels)\n",
    "    \n",
    "    # Assign values for new ROIs (continuing from the end of the Schaefer atlas)\n",
    "    roi_values = {'pIPS': 201, 'LO': 202}\n",
    "    overlap_info = {}\n",
    "    \n",
    "    # Process each ROI\n",
    "    for roi_name, roi_img in rois.items():\n",
    "        # Get ROI data and create mask\n",
    "        roi_data = roi_img.get_fdata()\n",
    "        roi_mask = roi_data > 0\n",
    "        \n",
    "        # Find overlapping parcels\n",
    "        overlap_mask = (atlas_data > 0) & roi_mask\n",
    "        overlapping_labels = np.unique(atlas_data[overlap_mask])\n",
    "        overlapping_labels = overlapping_labels[overlapping_labels > 0]\n",
    "        \n",
    "        # Get number of voxels in overlap\n",
    "        overlap_voxels = {}\n",
    "        for label in overlapping_labels:\n",
    "            label_mask = (atlas_data == label) & roi_mask\n",
    "            overlap_voxels[int(label)] = np.sum(label_mask)\n",
    "        \n",
    "        # Store overlap information\n",
    "        overlap_info[roi_name] = {\n",
    "            'overlapping_labels': overlapping_labels.tolist(),\n",
    "            'overlap_voxels': overlap_voxels\n",
    "        }\n",
    "        \n",
    "        print(f\"{roi_name} overlaps with {len(overlapping_labels)} atlas parcels\")\n",
    "        for label, voxels in overlap_voxels.items():\n",
    "            label_idx = int(label) - 1  # Convert to 0-indexed\n",
    "            if 0 <= label_idx < len(atlas_labels):\n",
    "                label_name = atlas_labels[label_idx]\n",
    "                label_name = label_name.decode('utf-8') if isinstance(label_name, bytes) else str(label_name)\n",
    "                print(f\"  Label {label} ({label_name}): {voxels} voxels\")\n",
    "        \n",
    "        # Remove overlapping parcels from the atlas\n",
    "        for label in overlapping_labels:\n",
    "            label_mask = (modified_atlas_data == label) & roi_mask\n",
    "            modified_atlas_data[label_mask] = 0\n",
    "        \n",
    "        # Add ROI with new label\n",
    "        modified_atlas_data[roi_mask] = roi_values[roi_name]\n",
    "        \n",
    "        # Add new label name\n",
    "        new_labels.append(f\"Wang_{roi_name}\")\n",
    "    \n",
    "    # Create the modified atlas\n",
    "    modified_atlas_img = nib.Nifti1Image(modified_atlas_data, atlas_img.affine, atlas_img.header)\n",
    "    merged_atlas_file = f'{results_dir}/schaefer_wang_merged.nii.gz'\n",
    "    nib.save(modified_atlas_img, merged_atlas_file)\n",
    "    print(f\"Saved merged atlas to: {merged_atlas_file}\")\n",
    "    \n",
    "    # Save new labels array\n",
    "    np.save(f'{results_dir}/merged_atlas_labels.npy', new_labels)\n",
    "    \n",
    "    # Return atlas information\n",
    "    return {\n",
    "        'atlas_img': atlas_img,\n",
    "        'merged_atlas_img': modified_atlas_img,\n",
    "        'atlas_labels': atlas_labels,\n",
    "        'merged_labels': new_labels,\n",
    "        'roi_values': roi_values,\n",
    "        'overlap_info': overlap_info\n",
    "    }\n",
    "\n",
    "# Create the merged atlas\n",
    "merged_atlas = create_merged_atlas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Connectivity Fingerprint Analysis\n",
    "\n",
    "To characterize the broader network organization, we implemented a connectivity fingerprint analysis examining connectivity between our seed regions and all regions in our merged atlas. This approach revealed:\n",
    "\n",
    "- High mean connectivity vector correlation between pIPS and LO (0.724 ± 0.222), indicating overall similar connectivity patterns\n",
    "- 94 regions showing significant differences via bootstrap analysis (95% confidence intervals)\n",
    "- 67 regions with high reliability via Leave-One-Out Cross-Validation\n",
    "- 60 regions significant with combined criteria (meeting both bootstrap and LOOCV thresholds)\n",
    "\n",
    "These results suggest that while the dorsal and ventral pathways have largely similar connectivity profiles (high correlation), they maintain distinct connections with specific brain regions, reflecting their specialized processing roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fingerprint connectivity analysis using the merged atlas\n",
    "def analyze_connectivity_and_save_results(analysis_type='fc'):\n",
    "    \"\"\"\n",
    "    Analyze connectivity and save results as CSV using merged atlas\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    analysis_type : str\n",
    "        Type of analysis to perform ('fc' for functional connectivity)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        DataFrame with results and dictionary with ROI data for visualization\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ANALYZING {analysis_type.upper()} CONNECTIVITY AND SAVING RESULTS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Load merged atlas instead of fetching standard Schaefer atlas\n",
    "    merged_atlas_file = f'{results_dir}/schaefer_wang_merged.nii.gz'\n",
    "    merged_labels_file = f'{results_dir}/merged_atlas_labels.npy'\n",
    "    \n",
    "    if not os.path.exists(merged_atlas_file) or not os.path.exists(merged_labels_file):\n",
    "        print(\"Error: Merged atlas files not found. Please run merge_atlas.py first.\")\n",
    "        return None\n",
    "    \n",
    "    atlas_img = nib.load(merged_atlas_file)\n",
    "    atlas_labels = np.load(merged_labels_file, allow_pickle=True)\n",
    "    \n",
    "    print(f\"Loaded merged atlas with {len(atlas_labels)} parcels\")\n",
    "    \n",
    "    # Load subject info\n",
    "    sub_info = pd.read_csv(sub_info_path)\n",
    "    subjects = sub_info[sub_info['group'] == 'control']['sub'].tolist()\n",
    "    \n",
    "    # Exclude sub-084 as specified\n",
    "    if 'sub-084' in subjects:\n",
    "        subjects.remove('sub-084')\n",
    "        print(\"Excluded sub-084 from analysis\")\n",
    "        \n",
    "    print(f\"Found {len(subjects)} control subjects\")\n",
    "    \n",
    "    # Define ROIs and hemispheres\n",
    "    rois = ['pIPS', 'LO']\n",
    "    hemispheres = ['left', 'right']\n",
    "    \n",
    "    # Setup atlas masker\n",
    "    masker = NiftiLabelsMasker(labels_img=atlas_img, standardize=False)\n",
    "    \n",
    "    # Load and process subject data\n",
    "    subject_data = []\n",
    "    for sub in subjects:\n",
    "        sub_conn = {}\n",
    "        \n",
    "        for roi in rois:\n",
    "            # Initialize arrays to hold combined data\n",
    "            combined_data = None\n",
    "            hemi_count = 0\n",
    "            \n",
    "            for hemisphere in hemispheres:\n",
    "                fc_file = f'{study_dir}/{sub}/ses-01/derivatives/fc_mni/{sub}_{roi}_{hemisphere}_loc_{analysis_type}_mni.nii.gz'\n",
    "                \n",
    "                if os.path.exists(fc_file):\n",
    "                    try:\n",
    "                        # Load the FC map\n",
    "                        fc_img = nib.load(fc_file)\n",
    "                        \n",
    "                        # Extract ROI values using atlas\n",
    "                        fc_values = masker.fit_transform(fc_img)[0]\n",
    "                        \n",
    "                        # Add to combined data\n",
    "                        if combined_data is None:\n",
    "                            combined_data = fc_values\n",
    "                        else:\n",
    "                            combined_data += fc_values\n",
    "                        \n",
    "                        hemi_count += 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {fc_file}: {e}\")\n",
    "            \n",
    "            # Average the data if we have at least one hemisphere\n",
    "            if hemi_count > 0:\n",
    "                sub_conn[roi] = combined_data / hemi_count\n",
    "        \n",
    "        # Only include subjects with both ROIs\n",
    "        if len(sub_conn) == len(rois):\n",
    "            subject_data.append(sub_conn)\n",
    "    \n",
    "    n_valid_subjects = len(subject_data)\n",
    "    print(f\"Successfully loaded data for {n_valid_subjects} subjects\")\n",
    "    \n",
    "    if n_valid_subjects < 3:\n",
    "        print(\"Not enough valid subjects. Analysis cannot proceed.\")\n",
    "        return None\n",
    "    \n",
    "    # Initialize arrays for analysis\n",
    "    n_rois = len(atlas_labels)\n",
    "    pips_fc = np.zeros((n_valid_subjects, n_rois))\n",
    "    lo_fc = np.zeros((n_valid_subjects, n_rois))\n",
    "    \n",
    "    # Fill arrays with connectivity data\n",
    "    for i, subj_data in enumerate(subject_data):\n",
    "        pips_fc[i, :] = subj_data['pIPS']\n",
    "        lo_fc[i, :] = subj_data['LO']\n",
    "    \n",
    "    # Calculate connectivity vector correlation for each subject\n",
    "    vector_correlations = np.zeros(n_valid_subjects)\n",
    "    for i in range(n_valid_subjects):\n",
    "        vector_correlations[i] = np.corrcoef(pips_fc[i, :], lo_fc[i, :])[0, 1]\n",
    "    \n",
    "    print(f\"Mean connectivity vector correlation: {np.mean(vector_correlations):.3f} ± {np.std(vector_correlations):.3f}\")\n",
    "    \n",
    "    # Calculate mean connectivity profiles\n",
    "    mean_pips = np.mean(pips_fc, axis=0)\n",
    "    mean_lo = np.mean(lo_fc, axis=0)\n",
    "    \n",
    "    # Remove self-connectivity\n",
    "    # Get indices for Wang ROIs in the merged atlas\n",
    "    wang_pips_idx = next((i for i, label in enumerate(atlas_labels) if 'Wang_pIPS' in str(label)), None)\n",
    "    wang_lo_idx = next((i for i, label in enumerate(atlas_labels) if 'Wang_LO' in str(label)), None)\n",
    "    \n",
    "    # Print which indices are being excluded\n",
    "    if wang_pips_idx is not None:\n",
    "        print(f\"Excluding Wang_pIPS (index {wang_pips_idx}) from connectivity analysis\")\n",
    "    if wang_lo_idx is not None:\n",
    "        print(f\"Excluding Wang_LO (index {wang_lo_idx}) from connectivity analysis\")\n",
    "    \n",
    "    # Create mask for non-self connections (all True except at Wang ROI indices)\n",
    "    mask = np.ones(len(atlas_labels), dtype=bool)\n",
    "    if wang_pips_idx is not None:\n",
    "        mask[wang_pips_idx] = False\n",
    "    if wang_lo_idx is not None:\n",
    "        mask[wang_lo_idx] = False\n",
    "    \n",
    "    # Apply mask to connectivity data\n",
    "    pips_fc_masked = pips_fc[:, mask]\n",
    "    lo_fc_masked = lo_fc[:, mask]\n",
    "    \n",
    "    # Get the masked atlas labels\n",
    "    atlas_labels_masked = [label for i, label in enumerate(atlas_labels) if mask[i]]\n",
    "    \n",
    "    # Update mean connectivity with masked data\n",
    "    mean_pips_masked = np.mean(pips_fc_masked, axis=0)\n",
    "    mean_lo_masked = np.mean(lo_fc_masked, axis=0)\n",
    "    \n",
    "    # Get the new number of ROIs after masking\n",
    "    n_masked_rois = np.sum(mask)\n",
    "    print(f\"Analyzing {n_masked_rois} ROIs after excluding Wang ROIs\")\n",
    "    \n",
    "    # Calculate difference profile with masked data\n",
    "    diff_profile = mean_pips_masked - mean_lo_masked\n",
    "    abs_diff_profile = np.abs(diff_profile)\n",
    "    \n",
    "    # Run bootstrap analysis with masked data\n",
    "    print(\"Performing bootstrap analysis...\")\n",
    "    n_boots = 10000\n",
    "    boot_diffs = np.zeros((n_boots, n_masked_rois))\n",
    "    \n",
    "    for i in range(n_boots):\n",
    "        # Resample subjects with replacement\n",
    "        boot_idx = resample(range(n_valid_subjects), replace=True, n_samples=n_valid_subjects)\n",
    "        \n",
    "        # Calculate mean difference for this bootstrap sample\n",
    "        boot_pips = np.mean(pips_fc_masked[boot_idx, :], axis=0)\n",
    "        boot_lo = np.mean(lo_fc_masked[boot_idx, :], axis=0)\n",
    "        boot_diffs[i, :] = boot_pips - boot_lo\n",
    "    \n",
    "    # Calculate confidence intervals\n",
    "    ci_lower = np.percentile(boot_diffs, 2.5, axis=0)\n",
    "    ci_upper = np.percentile(boot_diffs, 97.5, axis=0)\n",
    "    \n",
    "    # Identify significant differences (95% CI doesn't cross zero)\n",
    "    sig_boot = (ci_lower > 0) | (ci_upper < 0)\n",
    "    sig_boot_count = np.sum(sig_boot)\n",
    "    print(f\"Found {sig_boot_count} ROIs with significant differences via bootstrap\")\n",
    "    \n",
    "    # Leave-One-Out Cross-Validation for reliability\n",
    "    print(\"Performing Leave-One-Out Cross-Validation...\")\n",
    "    loo_reliability = np.zeros(n_masked_rois)\n",
    "    \n",
    "    for left_out in range(n_valid_subjects):\n",
    "        # Create the training set (all subjects except the left out one)\n",
    "        train_idx = list(range(n_valid_subjects))\n",
    "        train_idx.remove(left_out)\n",
    "        \n",
    "        # Calculate mean training differences\n",
    "        train_diff = np.mean(pips_fc_masked[train_idx, :] - lo_fc_masked[train_idx, :], axis=0)\n",
    "        \n",
    "        # Test on left out subject\n",
    "        test_diff = pips_fc_masked[left_out, :] - lo_fc_masked[left_out, :]\n",
    "        \n",
    "        # Calculate consistency of signs between training and test\n",
    "        loo_reliability += (np.sign(train_diff) == np.sign(test_diff)).astype(float)\n",
    "    \n",
    "    # Convert to proportion of consistent predictions\n",
    "    loo_reliability = loo_reliability / n_valid_subjects\n",
    "    \n",
    "    # Set threshold at 75% consistency\n",
    "    loo_threshold = 0.75\n",
    "    sig_loo = loo_reliability >= loo_threshold\n",
    "    sig_loo_count = np.sum(sig_loo)\n",
    "    \n",
    "    print(f\"LOO reliability threshold: {loo_threshold:.3f}\")\n",
    "    print(f\"Found {sig_loo_count} ROIs with high reliability via LOO\")\n",
    "    \n",
    "    # Combine significance\n",
    "    sig_combined = sig_boot & sig_loo\n",
    "    sig_combined_count = np.sum(sig_combined)\n",
    "    print(f\"Found {sig_combined_count} ROIs significant with combined criteria\")\n",
    "    \n",
    "    # Helper function to get ROI name\n",
    "    def get_roi_name(roi_id):\n",
    "        try:\n",
    "            idx = int(roi_id) - 1\n",
    "            if 0 <= idx < len(atlas_labels):\n",
    "                label = atlas_labels[idx]\n",
    "                if isinstance(label, bytes):\n",
    "                    label = label.decode('utf-8')\n",
    "                return label\n",
    "        except:\n",
    "            pass\n",
    "        return f\"ROI_{roi_id}\"\n",
    "    \n",
    "    # Create a map from masked indices to original atlas indices\n",
    "    original_indices = np.where(mask)[0]\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        'ROI_ID': original_indices + 1,  # Convert to 1-based indexing\n",
    "        'ROI_Name': [get_roi_name(i+1) for i in original_indices],\n",
    "        'pIPS_Connectivity': mean_pips_masked,\n",
    "        'LO_Connectivity': mean_lo_masked,\n",
    "        'Difference': diff_profile,\n",
    "        'Abs_Difference': abs_diff_profile,\n",
    "        'CI_Lower': ci_lower,\n",
    "        'CI_Upper': ci_upper,\n",
    "        'LOO_Reliability': loo_reliability,\n",
    "        'Bootstrap_Significant': sig_boot,\n",
    "        'LOO_Significant': sig_loo,\n",
    "        'Combined_Significant': sig_combined,\n",
    "        'Direction': np.where(diff_profile > 0, 'pIPS > LO', 'LO > pIPS')\n",
    "    })\n",
    "    \n",
    "    # Save results to CSV\n",
    "    csv_path = f'{output_dir}/bilateral_{analysis_type}_connectivity_fingerprint_results.csv'\n",
    "    results_df.to_csv(csv_path, index=False)\n",
    "    print(f\"Results saved to {csv_path}\")\n",
    "    \n",
    "    # Create roi_data dictionary for visualization\n",
    "    roi_data = {\n",
    "        'mean_pips': mean_pips_masked,\n",
    "        'mean_lo': mean_lo_masked,\n",
    "        'diff_profile': diff_profile,\n",
    "        'ci_lower': ci_lower,\n",
    "        'ci_upper': ci_upper,\n",
    "        'sig_combined': sig_combined\n",
    "    }\n",
    "    \n",
    "    return results_df, roi_data\n",
    "\n",
    "# Run analysis for FC\n",
    "fc_results, fc_data = analyze_connectivity_and_save_results(analysis_type='fc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Statistical Analysis of Connectivity Asymmetry\n",
    "\n",
    "To further characterize the differences between dorsal and ventral connectivity profiles, we performed statistical analyses comparing the number and strength of connections for each pathway.\n",
    "\n",
    "The connectivity asymmetry analysis revealed a striking imbalance in the number of connections favoring different pathways:\n",
    "\n",
    "- pIPS showed stronger connectivity with 50 regions, while LO showed stronger connectivity with only 10 regions\n",
    "- This asymmetry was highly significant (χ² = 26.67, p < 0.001)\n",
    "- Mean strength of connections did not differ significantly (pIPS: 0.0359 ± 0.0200, LO: 0.0266 ± 0.0128, p = 0.30)\n",
    "- Total connectivity strength was substantially higher for pIPS (pIPS/LO ratio: 6.76)\n",
    "\n",
    "These results support our hypotheses that pIPS (dorsal pathway) maintains significantly more connections throughout the brain compared to LO (ventral pathway), though the strength of individual connections is similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze dorsal-ventral asymmetry in connectivity\n",
    "def analyze_connectivity_asymmetry(analysis_type='fc'):\n",
    "    \"\"\"\n",
    "    Analyze the asymmetry in number and strength of connections for pIPS and LO regions.\n",
    "    Tests whether pIPS has significantly more and stronger connections than LO.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    analysis_type : str\n",
    "        Type of analysis to perform ('fc' for functional connectivity)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing results of the asymmetry analysis\n",
    "    \"\"\"\n",
    "    # Define paths\n",
    "    results_dir = '/user_data/csimmon2/git_repos/ptoc/results'\n",
    "    connectivity_dir = f'{results_dir}/connectivity_comparison'\n",
    "    output_dir = f'{results_dir}/connectivity_asymmetry'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ANALYZING {analysis_type.upper()} ASYMMETRY IN CONNECTIONS BETWEEN pIPS AND LO...\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Load connectivity results from CSV\n",
    "    results_path = f'{connectivity_dir}/bilateral_{analysis_type}_connectivity_fingerprint_results.csv'\n",
    "    if not os.path.exists(results_path):\n",
    "        print(f\"Error: Could not find {analysis_type.upper()} connectivity results at {results_path}\")\n",
    "        return None\n",
    "    \n",
    "    results_df = pd.read_csv(results_path)\n",
    "    print(f\"Loaded {analysis_type.upper()} connectivity data with {len(results_df)} ROIs\")\n",
    "    \n",
    "    # Filter to significant regions according to the combined criteria\n",
    "    sig_regions = results_df[results_df['Combined_Significant'] == True]\n",
    "    print(f\"Found {len(sig_regions)} significant ROIs\")\n",
    "    \n",
    "    # Count regions by direction\n",
    "    pips_stronger_count = sum(sig_regions['Direction'] == 'pIPS > LO')\n",
    "    lo_stronger_count = sum(sig_regions['Direction'] == 'LO > pIPS')\n",
    "    \n",
    "    print(f\"pIPS > LO: {pips_stronger_count} regions\")\n",
    "    print(f\"LO > pIPS: {lo_stronger_count} regions\")\n",
    "    \n",
    "    # 1. Chi-square test for number of connections\n",
    "    observed = np.array([pips_stronger_count, lo_stronger_count])\n",
    "    expected = np.array([observed.sum()/2, observed.sum()/2])  # Expected under null hypothesis of equal distribution\n",
    "    \n",
    "    chi2, p_value = stats.chisquare(observed, expected)\n",
    "    \n",
    "    print(\"\\nChi-square test for asymmetry in number of connections:\")\n",
    "    print(f\"Observed frequencies: pIPS > LO: {observed[0]}, LO > pIPS: {observed[1]}\")\n",
    "    print(f\"Expected frequencies: pIPS > LO: {expected[0]}, LO > pIPS: {expected[1]}\")\n",
    "    print(f\"Chi-square statistic: {chi2:.4f}\")\n",
    "    print(f\"p-value: {p_value:.8f}\")\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(\"The asymmetry in number of connections is statistically significant.\")\n",
    "        if pips_stronger_count > lo_stronger_count:\n",
    "            print(f\"pIPS has significantly more connections than LO (p {'< .001' if p_value < 0.001 else f'= {p_value:.4f}'}).\")\n",
    "        else:\n",
    "            print(f\"LO has significantly more connections than pIPS (p {'< .001' if p_value < 0.001 else f'= {p_value:.4f}'}).\")\n",
    "    else:\n",
    "        print(\"The asymmetry in number of connections is not statistically significant.\")\n",
    "    \n",
    "    # 2. Extract connectivity strength values for significant regions\n",
    "    pips_stronger = np.abs(sig_regions[sig_regions['Direction'] == 'pIPS > LO']['Difference'].values)\n",
    "    lo_stronger = np.abs(sig_regions[sig_regions['Direction'] == 'LO > pIPS']['Difference'].values)\n",
    "    \n",
    "    pips_mean = np.mean(pips_stronger) if len(pips_stronger) > 0 else 0\n",
    "    pips_std = np.std(pips_stronger) if len(pips_stronger) > 0 else 0\n",
    "    lo_mean = np.mean(lo_stronger) if len(lo_stronger) > 0 else 0\n",
    "    lo_std = np.std(lo_stronger) if len(lo_stronger) > 0 else 0\n",
    "    \n",
    "    print(f\"\\nMean strength of pIPS-dominant connections: {pips_mean:.4f} ± {pips_std:.4f}\")\n",
    "    print(f\"Mean strength of LO-dominant connections: {lo_mean:.4f} ± {lo_std:.4f}\")\n",
    "    \n",
    "    # Mann-Whitney U test for comparing strength distributions (better for unequal sample sizes)\n",
    "    u_stat = None\n",
    "    p_value_mw = None\n",
    "    \n",
    "    if len(pips_stronger) > 0 and len(lo_stronger) > 0:\n",
    "        u_stat, p_value_mw = stats.mannwhitneyu(pips_stronger, lo_stronger, alternative='two-sided')\n",
    "        \n",
    "        print(\"\\nMann-Whitney U test for connection strength differences:\")\n",
    "        print(f\"U statistic: {u_stat}\")\n",
    "        print(f\"p-value: {p_value_mw:.8f}\")\n",
    "        \n",
    "        if p_value_mw < 0.05:\n",
    "            if np.median(pips_stronger) > np.median(lo_stronger):\n",
    "                print(f\"pIPS-dominant connections are significantly stronger than LO-dominant connections (p {'< .001' if p_value_mw < 0.001 else f'= {p_value_mw:.4f}'}).\")\n",
    "            else:\n",
    "                print(f\"LO-dominant connections are significantly stronger than pIPS-dominant connections (p {'< .001' if p_value_mw < 0.001 else f'= {p_value_mw:.4f}'}).\")\n",
    "        else:\n",
    "            print(\"No significant difference in connection strength between pIPS-dominant and LO-dominant regions.\")\n",
    "        \n",
    "        # Also calculate total connectivity strength\n",
    "        pips_total = np.sum(pips_stronger)\n",
    "        lo_total = np.sum(lo_stronger)\n",
    "        print(\"\\nTotal connectivity strength:\")\n",
    "        print(f\"pIPS: {pips_total:.4f} (across {len(pips_stronger)} connections)\")\n",
    "        print(f\"LO: {lo_total:.4f} (across {len(lo_stronger)} connections)\")\n",
    "        if lo_total > 0:\n",
    "            print(f\"Ratio pIPS/LO: {pips_total/lo_total:.2f}\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot number of connections\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.bar(['pIPS > LO', 'LO > pIPS'], [pips_stronger_count, lo_stronger_count], \n",
    "            color=['#4ac0c0', '#ff9b83'])\n",
    "    plt.title(f'{analysis_type.upper()}: Number of Significant Connections')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "    # Plot connection strengths with boxplot if both groups have data\n",
    "    if len(pips_stronger) > 0 and len(lo_stronger) > 0:\n",
    "        plt.subplot(1, 3, 2)\n",
    "        boxplot = plt.boxplot([pips_stronger, lo_stronger], labels=['pIPS > LO', 'LO > pIPS'], \n",
    "                  patch_artist=True)\n",
    "        \n",
    "        # Set colors to match the bar plot\n",
    "        boxplot['boxes'][0].set_facecolor('#4ac0c0')\n",
    "        boxplot['boxes'][1].set_facecolor('#ff9b83')\n",
    "        \n",
    "        plt.title(f'{analysis_type.upper()}: Connection Strength Distribution')\n",
    "        plt.ylabel('Absolute Connectivity Difference')\n",
    "        \n",
    "        # Add median lines to the plot\n",
    "        plt.axhline(y=np.median(pips_stronger), color='#4ac0c0', linestyle='--', alpha=0.5)\n",
    "        plt.axhline(y=np.median(lo_stronger), color='#ff9b83', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        # Plot violin plot to better visualize distributions\n",
    "        plt.subplot(1, 3, 3)\n",
    "        violin = plt.violinplot([pips_stronger, lo_stronger], showmeans=True)\n",
    "        \n",
    "        # Customize violin plot colors\n",
    "        for i, pc in enumerate(violin['bodies']):\n",
    "            if i == 0:\n",
    "                pc.set_facecolor('#4ac0c0')\n",
    "            else:\n",
    "                pc.set_facecolor('#ff9b83')\n",
    "        \n",
    "        plt.xticks([1, 2], ['pIPS > LO', 'LO > pIPS'])\n",
    "        plt.title(f'{analysis_type.upper()}: Distribution of Connection Strengths')\n",
    "        plt.ylabel('Absolute Connectivity Difference')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/{analysis_type}_connectivity_asymmetry_analysis.png', dpi=300)\n",
    "    \n",
    "    # Calculate median values safely\n",
    "    pips_median = np.median(pips_stronger) if len(pips_stronger) > 0 else None\n",
    "    lo_median = np.median(lo_stronger) if len(lo_stronger) > 0 else None\n",
    "    \n",
    "    return {\n",
    "        'analysis_type': analysis_type,\n",
    "        'number_test': {'chi2': chi2, 'p_value': p_value},\n",
    "        'strength_test': {'u_stat': u_stat, 'p_value': p_value_mw},\n",
    "        'counts': {'pips_stronger': pips_stronger_count, 'lo_stronger': lo_stronger_count},\n",
    "        'strengths': {'pips_mean': pips_mean,\n",
    "                     'lo_mean': lo_mean,\n",
    "                     'pips_median': pips_median,\n",
    "                     'lo_median': lo_median,\n",
    "                     'pips_total': np.sum(pips_stronger),\n",
    "                     'lo_total': np.sum(lo_stronger)}\n",
    "    }\n",
    "\n",
    "# Run asymmetry analysis for FC\n",
    "asymmetry_results = analyze_connectivity_asymmetry(analysis_type='fc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Brain Connectivity\n",
    "def visualize_brain_connectivity(analysis_type='fc'):\n",
    "    \"\"\"\n",
    "    Create brain visualization for specified data type using saved CSV results\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    analysis_type : str\n",
    "        Type of analysis to visualize ('fc' for functional connectivity)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame containing significant ROIs\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"CREATING {analysis_type.upper()} BRAIN CONNECTIVITY FIGURE FROM SAVED DATA\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Load merged atlas instead of the standard Schaefer atlas\n",
    "    merged_atlas_file = f'{results_dir}/schaefer_wang_merged.nii.gz'\n",
    "    merged_labels_file = f'{results_dir}/merged_atlas_labels.npy'\n",
    "    \n",
    "    if not os.path.exists(merged_atlas_file) or not os.path.exists(merged_labels_file):\n",
    "        print(\"Error: Merged atlas files not found. Please run merge_atlas.py first.\")\n",
    "        return None\n",
    "    \n",
    "    # Load merged atlas\n",
    "    atlas_img = nib.load(merged_atlas_file)\n",
    "    atlas_labels = np.load(merged_labels_file, allow_pickle=True)\n",
    "    print(f\"Loaded merged atlas with {len(atlas_labels)} regions\")\n",
    "    \n",
    "    # Load saved connectivity results\n",
    "    csv_path = f'{connectivity_dir}/bilateral_{analysis_type}_connectivity_fingerprint_results.csv'\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"Error: Could not find {analysis_type.upper()} results at {csv_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Load the data\n",
    "    results_df = pd.read_csv(csv_path)\n",
    "    print(f\"Loaded {analysis_type.upper()} data with {len(results_df)} ROIs\")\n",
    "    \n",
    "    # Filter to significant regions according to the combined criteria\n",
    "    sig_df = results_df[results_df['Combined_Significant'] == True].copy()\n",
    "    sig_df['Hemisphere'] = sig_df['ROI_Name'].apply(lambda x: 'L' if 'LH' in x else 'R' if 'RH' in x else 'X')\n",
    "    \n",
    "    print(f\"Found {len(sig_df)} significant ROIs\")\n",
    "    \n",
    "    if len(sig_df) > 0:\n",
    "        # Get coordinates for all ROIs from the merged atlas\n",
    "        all_coords = plotting.find_parcellation_cut_coords(atlas_img)\n",
    "        \n",
    "        # Prepare node properties for significant ROIs\n",
    "        sig_coords = []\n",
    "        sig_colors = []\n",
    "        sig_sizes = []\n",
    "        \n",
    "        for _, row in sig_df.iterrows():\n",
    "            roi_idx = row['ROI_ID'] - 1  # Convert to 0-based index\n",
    "            sig_coords.append(all_coords[roi_idx])\n",
    "            sig_colors.append('#4ac0c0' if row['Difference'] > 0 else '#ff9b83')\n",
    "            sig_sizes.append(abs(row['Difference']) * 40 + 30)\n",
    "        \n",
    "        # Add seed nodes\n",
    "        seed_coords = [\n",
    "            [-26, -68, 48], [26, -68, 48],  # pIPS L/R\n",
    "            [-42, -82, -8], [42, -82, -8]    # LO L/R\n",
    "        ]\n",
    "        all_coords = sig_coords + seed_coords\n",
    "        all_colors = sig_colors + ['#4ac0c0', '#4ac0c0', '#ff9b83', '#ff9b83']\n",
    "        all_sizes = sig_sizes + [200, 200, 200, 200]\n",
    "        \n",
    "        # Create adjacency matrix\n",
    "        adjacency = np.zeros((len(all_coords), len(all_coords)))\n",
    "        for i, row in enumerate(sig_df.itertuples()):\n",
    "            tgt_offset = 0 if row.Difference > 0 else 2\n",
    "            tgt_idx = len(sig_coords) + tgt_offset + (0 if row.Hemisphere == 'L' else 1)\n",
    "            adjacency[i, tgt_idx] = adjacency[tgt_idx, i] = 1\n",
    "        \n",
    "        # Create network plot\n",
    "        fig = plt.figure(figsize=(18, 16))\n",
    "        plotting.plot_connectome(\n",
    "            adjacency, all_coords,\n",
    "            node_color=all_colors,\n",
    "            node_size=all_sizes,\n",
    "            edge_kwargs={'color': 'gray', 'alpha': 0.3, 'linewidth': 1.0},\n",
    "            display_mode='lzry',\n",
    "            title=f'{analysis_type.upper()}: All {len(sig_df)} Significant ROIs',\n",
    "            figure=fig\n",
    "        )\n",
    "        \n",
    "        # Save the figure\n",
    "        output_path = f'{connectivity_dir}/{analysis_type}_brain_connectivity.png'\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        print(f\"Brain network visualization complete for {analysis_type}\")\n",
    "        print(f\"Showing {len(sig_df)} significant ROIs connected to pIPS and LO seed regions\")\n",
    "        print(f\"Figure saved to {output_path}\")\n",
    "    else:\n",
    "        print(f\"No significant ROIs found for {analysis_type}. Cannot create brain visualization.\")\n",
    "    \n",
    "    return sig_df\n",
    "\n",
    "# Create brain connectivity visualization\n",
    "sig_roi_df = visualize_brain_connectivity(analysis_type='fc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Anatomical Organization\n",
    "def visualize_anatomical_organization(results_df, roi_data, analysis_type='fc'):\n",
    "    \"\"\"\n",
    "    Create anatomical organization visualization\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results_df : pandas.DataFrame\n",
    "        DataFrame containing connectivity results\n",
    "    roi_data : dict\n",
    "        Dictionary containing ROI data for visualization\n",
    "    analysis_type : str\n",
    "        Type of analysis to visualize ('fc' for functional connectivity)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"CREATING {analysis_type.upper()} ANATOMICAL ORGANIZATION VISUALIZATION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Extract data from parameters\n",
    "    mean_pips = roi_data['mean_pips']\n",
    "    mean_lo = roi_data['mean_lo']\n",
    "    diff_profile = roi_data['diff_profile']\n",
    "    ci_lower = roi_data['ci_lower']\n",
    "    ci_upper = roi_data['ci_upper']\n",
    "    sig_combined = roi_data['sig_combined']\n",
    "    \n",
    "    # Function to map ROI names to anatomical lobes\n",
    "    def map_to_anatomical_lobe(roi_name):\n",
    "        # Special handling for Wang atlas ROIs\n",
    "        if 'Wang_pIPS' in roi_name:\n",
    "            return 'Parietal'\n",
    "        elif 'Wang_LO' in roi_name:\n",
    "            return 'Temporal'\n",
    "        \n",
    "        # Map Schaefer networks to anatomical regions\n",
    "        if 'Vis' in roi_name:\n",
    "            return 'Occipital'\n",
    "        elif 'SomMot' in roi_name:\n",
    "            return 'Somatomotor'\n",
    "        elif 'DorsAttn' in roi_name:\n",
    "            if 'Par' in roi_name or 'IPL' in roi_name or 'IPS' in roi_name:\n",
    "                return 'Parietal'\n",
    "            elif 'Temp' in roi_name or 'MT' in roi_name:\n",
    "                return 'Temporal'\n",
    "            else:\n",
    "                return 'Parietal'\n",
    "        elif 'SalVentAttn' in roi_name:\n",
    "            if 'Ins' in roi_name:\n",
    "                return 'Insular'\n",
    "            elif 'Cing' in roi_name or 'ACC' in roi_name:\n",
    "                return 'Cingulate'\n",
    "            elif 'Temp' in roi_name:\n",
    "                return 'Temporal'\n",
    "            elif 'Par' in roi_name:\n",
    "                return 'Parietal'\n",
    "            else:\n",
    "                return 'Frontal'\n",
    "        elif 'Limbic' in roi_name:\n",
    "            if 'Temp' in roi_name:\n",
    "                return 'Temporal'\n",
    "            else:\n",
    "                return 'Frontal'\n",
    "        elif 'Cont' in roi_name:\n",
    "            if 'Par' in roi_name or 'IPL' in roi_name or 'IPS' in roi_name:\n",
    "                return 'Parietal'\n",
    "            elif 'Temp' in roi_name or 'MT' in roi_name:\n",
    "                return 'Temporal'\n",
    "            elif 'Cing' in roi_name:\n",
    "                return 'Cingulate'\n",
    "            else:\n",
    "                return 'Frontal'\n",
    "        elif 'Default' in roi_name:\n",
    "            if 'Par' in roi_name:\n",
    "                return 'Parietal'\n",
    "            elif 'Temp' in roi_name:\n",
    "                return 'Temporal'\n",
    "            elif 'PCC' in roi_name or 'Cing' in roi_name:\n",
    "                return 'Cingulate'\n",
    "            else:\n",
    "                return 'Frontal'\n",
    "        else:\n",
    "            return 'Other'\n",
    "    \n",
    "    # Clean up ROI names for better labeling\n",
    "    def clean_roi_name(roi_name):\n",
    "        if isinstance(roi_name, bytes):\n",
    "            roi_name = roi_name.decode('utf-8')\n",
    "        \n",
    "        # Handle Wang ROIs\n",
    "        if 'Wang_' in roi_name:\n",
    "            return roi_name.replace('Wang_', '')\n",
    "        \n",
    "        # Remove common prefixes and suffixes\n",
    "        cleaned = roi_name.replace('7Networks_', '')\n",
    "        \n",
    "        # Extract just the region part, removing hemisphere and network\n",
    "        parts = cleaned.split('_')\n",
    "        if len(parts) > 2:\n",
    "            return parts[-2] + '_' + parts[-1]\n",
    "        elif len(parts) > 1:\n",
    "            return parts[-1]\n",
    "        else:\n",
    "            return cleaned\n",
    "    \n",
    "    # Determine hemisphere and base region for organizing pairs\n",
    "    def get_hemisphere_and_region(roi_name):\n",
    "        if isinstance(roi_name, bytes):\n",
    "            roi_name = roi_name.decode('utf-8')\n",
    "        \n",
    "        # Determine hemisphere \n",
    "        if 'LH' in roi_name:\n",
    "            hemisphere = 'L'\n",
    "        elif 'RH' in roi_name:\n",
    "            hemisphere = 'R'\n",
    "        else:\n",
    "            # For Wang ROIs or ROIs without clear hemisphere\n",
    "            if 'Wang_pIPS' in roi_name or 'Wang_LO' in roi_name:\n",
    "                hemisphere = 'X'  # Bilateral\n",
    "            else:\n",
    "                hemisphere = 'X'  # Unknown\n",
    "        \n",
    "        # Extract base region by removing hemisphere and number\n",
    "        cleaned = roi_name.replace('7Networks_', '')\n",
    "        cleaned = cleaned.replace('LH_', '').replace('RH_', '')\n",
    "        \n",
    "        # Handle Wang ROIs\n",
    "        if 'Wang_' in cleaned:\n",
    "            base_region = cleaned.replace('Wang_', '')\n",
    "        else:\n",
    "            # Remove trailing numbers which often differentiate regions\n",
    "            import re\n",
    "            base_region = re.sub(r'_\\d+$', '', cleaned)\n",
    "        \n",
    "        return hemisphere, base_region\n",
    "    \n",
    "    # Add anatomical lobe and hemisphere information to the dataframe\n",
    "    results_df['Anatomical_Lobe'] = results_df['ROI_Name'].apply(map_to_anatomical_lobe)\n",
    "    results_df['Clean_Name'] = results_df['ROI_Name'].apply(clean_roi_name)\n",
    "    \n",
    "    # Extract hemisphere and base region info\n",
    "    hemisphere_region = results_df['ROI_Name'].apply(get_hemisphere_and_region)\n",
    "    results_df['Hemisphere'] = [h for h, r in hemisphere_region]\n",
    "    results_df['Base_Region'] = [r for h, r in hemisphere_region]\n",
    "    \n",
    "    # Define a manual order for anatomical lobes\n",
    "    lobe_order = [\n",
    "        'Frontal',\n",
    "        'Somatomotor',\n",
    "        'Parietal',\n",
    "        'Temporal',\n",
    "        'Occipital',\n",
    "        'Insular',\n",
    "        'Cingulate',\n",
    "        'Other'\n",
    "    ]\n",
    "    \n",
    "    # Create a category for sorting based on the defined order\n",
    "    from pandas.api.types import CategoricalDtype\n",
    "    lobe_cat = pd.Categorical(results_df['Anatomical_Lobe'], categories=lobe_order, ordered=True)\n",
    "    results_df['Lobe_Sorted'] = lobe_cat\n",
    "    \n",
    "    # Define a consistent color palette for brain lobes\n",
    "    lobe_colors = {\n",
    "        'Frontal': '#3498db',     # Blue\n",
    "        'Somatomotor': '#f1c40f', # Yellow\n",
    "        'Parietal': '#e74c3c',    # Red\n",
    "        'Temporal': '#2ecc71',    # Green\n",
    "        'Occipital': '#9b59b6',   # Purple\n",
    "        'Insular': '#f39c12',     # Orange\n",
    "        'Cingulate': '#1abc9c',   # Turquoise\n",
    "        'Other': '#7f8c8d'        # Gray\n",
    "    }\n",
    "    \n",
    "    # Custom sorting function to group by lobe, then by base region, then by hemisphere (L then R)\n",
    "    def custom_sort(row):\n",
    "        lobe_idx = lobe_order.index(row['Anatomical_Lobe']) if row['Anatomical_Lobe'] in lobe_order else 999\n",
    "        hemi_idx = 0 if row['Hemisphere'] == 'L' else 1 if row['Hemisphere'] == 'R' else 2\n",
    "        return (lobe_idx, row['Base_Region'], hemi_idx)\n",
    "    \n",
    "    # Sort using the custom function\n",
    "    results_df['sort_key'] = results_df.apply(custom_sort, axis=1)\n",
    "    results_df_sorted = results_df.sort_values('sort_key')\n",
    "    \n",
    "    # Get the new order\n",
    "    sorted_indices = results_df_sorted.index.values\n",
    "    \n",
    "    # Extract reordered data\n",
    "    mean_pips_sorted = results_df_sorted['pIPS_Connectivity'].values\n",
    "    mean_lo_sorted = results_df_sorted['LO_Connectivity'].values\n",
    "    diff_profile_sorted = results_df_sorted['Difference'].values\n",
    "    sig_sorted = results_df_sorted['Combined_Significant'].values\n",
    "    \n",
    "    # Find peak (highest) and bottom (lowest) significant ROI in each lobe\n",
    "    significant_peak_bottom_indices = []\n",
    "    lobe_groups = results_df_sorted.groupby('Anatomical_Lobe')\n",
    "    \n",
    "    for lobe, group in lobe_groups:\n",
    "        if len(group) > 0:\n",
    "            # Get significant ROIs in this lobe\n",
    "            sig_group = group[group['Combined_Significant']]\n",
    "            \n",
    "            if len(sig_group) > 0:\n",
    "                # Get highest difference (most pIPS-preferring) significant ROI\n",
    "                peak_idx = sig_group['Difference'].idxmax()\n",
    "                significant_peak_bottom_indices.append(peak_idx)\n",
    "                \n",
    "                # Get lowest difference (most LO-preferring) significant ROI\n",
    "                bottom_idx = sig_group['Difference'].idxmin()\n",
    "                significant_peak_bottom_indices.append(bottom_idx)\n",
    "    \n",
    "    # Create the visualization\n",
    "    plt.figure(figsize=(18, 10))\n",
    "    \n",
    "    # Top subplot - connectivity profiles with lollipop plot, organized by lobe\n",
    "    ax1 = plt.subplot(2, 1, 1)\n",
    "    x = np.arange(len(mean_pips_sorted))\n",
    "    \n",
    "    # Get the y-limits for label positioning\n",
    "    y_min, y_max = min(min(mean_pips_sorted), min(mean_lo_sorted)), max(max(mean_pips_sorted), max(mean_lo_sorted))\n",
    "    y_range = y_max - y_min\n",
    "    \n",
    "    # Plot background colors for different lobes\n",
    "    current_lobe = None\n",
    "    start_idx = 0\n",
    "    \n",
    "    for i, idx in enumerate(sorted_indices):\n",
    "        lobe = results_df_sorted.iloc[i]['Anatomical_Lobe']\n",
    "        if lobe != current_lobe:\n",
    "            if current_lobe is not None:\n",
    "                # Mark the region for the previous lobe\n",
    "                plt.axvspan(start_idx - 0.5, i - 0.5, alpha=0.15, \n",
    "                          color=lobe_colors[current_lobe])\n",
    "                # Add label in the middle of the region with fixed top position\n",
    "                label_y = y_max + 0.15 * y_range  # Position labels above the data\n",
    "                plt.text((start_idx + i - 1) / 2, label_y, \n",
    "                       current_lobe, ha='center', fontsize=11, fontweight='bold')\n",
    "            current_lobe = lobe\n",
    "            start_idx = i\n",
    "    \n",
    "    # Add the last region\n",
    "    plt.axvspan(start_idx - 0.5, len(mean_pips_sorted) - 0.5, alpha=0.15, \n",
    "              color=lobe_colors[current_lobe])\n",
    "    plt.text((start_idx + len(mean_pips_sorted) - 1) / 2, y_max + 0.15 * y_range, \n",
    "           current_lobe, ha='center', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # Create lollipop plot - vertical lines connecting pIPS and LO points\n",
    "    for i in range(len(x)):\n",
    "        plt.plot([x[i], x[i]], [mean_pips_sorted[i], mean_lo_sorted[i]], color='gray', linestyle='-', linewidth=0.5)\n",
    "    \n",
    "    # Plot the points\n",
    "    plt.scatter(x, mean_pips_sorted, color='#4ac0c0', s=20, label='pIPS')\n",
    "    plt.scatter(x, mean_lo_sorted, color='#ff9b83', s=20, label='LO')\n",
    "    \n",
    "    # Set y-axis limits with padding for the labels\n",
    "    plt.ylim(y_min - 0.1 * y_range, y_max + 0.25 * y_range)\n",
    "    \n",
    "    plt.ylabel('Connectivity Strength')\n",
    "    plt.title(f'{analysis_type.upper()} ROI Connectivity Profiles (Organized by Anatomical Regions, L-R Pairs)')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.xlim(-0.5, len(mean_pips_sorted) - 0.5)\n",
    "    \n",
    "    # Bottom subplot - difference\n",
    "    ax2 = plt.subplot(2, 1, 2)\n",
    "    \n",
    "    # Create base bar plot for all bars\n",
    "    base_colors = ['#4ac0c0' if val > 0 else '#ff9b83' for val in diff_profile_sorted]\n",
    "    bars = plt.bar(x, diff_profile_sorted, color=base_colors)\n",
    "    \n",
    "    # Apply transparency to non-significant bars individually\n",
    "    for i, (bar, is_sig) in enumerate(zip(bars, sig_sorted)):\n",
    "        if not is_sig:\n",
    "            # Set alpha for non-significant bars only\n",
    "            bar.set_alpha(0.3)\n",
    "    \n",
    "    plt.axhline(y=0, color='black', linestyle='-')\n",
    "    \n",
    "    # Add a legend for significance\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='gray', alpha=1.0, label='Significant'),\n",
    "        Patch(facecolor='gray', alpha=0.3, label='Non-significant')\n",
    "    ]\n",
    "    plt.legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    # Add background colors for different lobes (same as top plot)\n",
    "    current_lobe = None\n",
    "    start_idx = 0\n",
    "    \n",
    "    for i, idx in enumerate(sorted_indices):\n",
    "        lobe = results_df_sorted.iloc[i]['Anatomical_Lobe']\n",
    "        if lobe != current_lobe:\n",
    "            if current_lobe is not None:\n",
    "                plt.axvspan(start_idx - 0.5, i - 0.5, alpha=0.15, \n",
    "                          color=lobe_colors[current_lobe])\n",
    "            current_lobe = lobe\n",
    "            start_idx = i\n",
    "    \n",
    "    # Add the last region\n",
    "    plt.axvspan(start_idx - 0.5, len(mean_pips_sorted) - 0.5, alpha=0.15, \n",
    "              color=lobe_colors[current_lobe])\n",
    "    \n",
    "    # Label only significant peak and bottom ROIs\n",
    "    for peak_idx in significant_peak_bottom_indices:\n",
    "        # Find position in sorted array\n",
    "        position = np.where(sorted_indices == peak_idx)[0][0]\n",
    "        roi_id = results_df.loc[peak_idx, 'ROI_ID']\n",
    "        roi_name = results_df.loc[peak_idx, 'Clean_Name']\n",
    "        diff_value = results_df.loc[peak_idx, 'Difference']\n",
    "        \n",
    "        # Add annotation for ROI\n",
    "        plt.annotate(f'ROI {roi_id}\\n{roi_name}',\n",
    "                   xy=(position, diff_value),\n",
    "                   xytext=(0, 20 if diff_value >= 0 else -25),\n",
    "                   textcoords='offset points',\n",
    "                   ha='center',\n",
    "                   va='bottom' if diff_value >= 0 else 'top',\n",
    "                   fontsize=8,\n",
    "                   fontweight='bold',\n",
    "                   bbox=dict(boxstyle='round,pad=0.3', fc='white', alpha=0.8),\n",
    "                   arrowprops=dict(arrowstyle='->', lw=1.5))\n",
    "    \n",
    "    plt.ylabel('pIPS - LO Difference')\n",
    "    plt.xlabel('ROI ID (Organized by Anatomical Region, L-R Pairs)')\n",
    "    plt.title(f'{analysis_type.upper()} Connectivity Difference (Transparent bars = non-significant)')\n",
    "    plt.xlim(-0.5, len(mean_pips_sorted) - 0.5)\n",
    "    plt.ylim(-0.2, 0.2)  # For FC analysis range from -0.2 to 0.2\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    fig_path = f'{output_dir}/{analysis_type}_anatomical_organization.png'\n",
    "    plt.savefig(fig_path, dpi=300)\n",
    "    \n",
    "    print(f\"Figure saved to {fig_path}\")\n",
    "    \n",
    "    # Print peak and bottom ROIs by lobe, only for significant ROIs\n",
    "    print(f\"\\n{analysis_type.upper()} significant peak and bottom ROIs by anatomical lobe:\")\n",
    "    for lobe, group in lobe_groups:\n",
    "        # Get significant ROIs in this lobe\n",
    "        sig_group = group[group['Combined_Significant']]\n",
    "        \n",
    "        if len(sig_group) > 0:\n",
    "            print(f\"\\n{lobe} Lobe:\")\n",
    "            \n",
    "            # Peak (pIPS-preferring) significant ROI\n",
    "            peak_row = sig_group.loc[sig_group['Difference'].idxmax()]\n",
    "            print(f\"  Peak (pIPS > LO): ROI {peak_row['ROI_ID']} ({peak_row['Clean_Name']}), \" + \n",
    "                 f\"Diff = {peak_row['Difference']:.3f}\")\n",
    "            \n",
    "            # Bottom (LO-preferring) significant ROI\n",
    "            bottom_row = sig_group.loc[sig_group['Difference'].idxmin()]\n",
    "            print(f\"  Bottom (LO > pIPS): ROI {bottom_row['ROI_ID']} ({bottom_row['Clean_Name']}), \" + \n",
    "                 f\"Diff = {bottom_row['Difference']:.3f}\")\n",
    "\n",
    "# Create anatomical organization visualization\n",
    "visualize_anatomical_organization(fc_results, fc_data, analysis_type='fc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Our findings provide robust evidence for an integrated network architecture supporting object recognition, with distinct connectivity profiles for dorsal (pIPS) and ventral (LO) visual pathways. The remarkably high spatial overlap (Dice coefficient = 0.91) between these pathways within individual subjects indicates they function as a singular network rather than separate systems. Despite this integration, we identified significant asymmetry in connectivity patterns, with the dorsal pathway showing stronger connectivity with five times more brain regions than the ventral pathway, though individual connection strengths remained similar. This pattern supports a model where the dorsal \"where/how\" pathway has broader influence across brain networks during object processing, while the ventral \"what\" pathway maintains more focused connectivity aligned with its specialized role in object recognition. These findings challenge traditional views of strictly segregated visual pathways and advance our understanding of how the brain integrates object recognition with spatial and action-related processing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brainiak_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
